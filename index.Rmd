---
title: "Methods for Supporting Decisions under Deep Uncertainty: A Systematic Bibliometric Review"
subtitle: "Analysis of 15,000+ Publications on Decision Support Methodologies"
author:
  - name: "Cory Whitney"
    affiliation: "University of Bonn, Germany"
    email: "cory.whitney@uni-bonn.de"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
abstract: |
  **Background:** Decision-making under deep uncertainty requires robust methodological approaches that can handle incomplete information and multiple stakeholder perspectives. This review systematically maps the methodological landscape across 15,000+ publications.
  
  **Methods:** We employ automated text mining, keyword classification, and bibliometric analysis to categorize methods, applications, and trends in decision support under uncertainty.
  
  **Keywords:** decision support, deep uncertainty, value of information, Bayesian analysis, stochastic optimization, multi-criteria decision analysis, systematic review, bibliometrics
bibliography: 
  - bib/01_introduction.bib
  - bib/01_1_review_synthesis.bib
  - bib/packages.bib
csl: apa.csl
link-citations: true
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
    number_sections: true
    theme: 
      version: 4
      bootswatch: flatly
    highlight: textmate
    code_folding: show
    code_download: false
    fig_width: 10
    fig_height: 6
    fig_caption: true
    keep_md: true
    df_print: kable
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}

# Global knitr options
knitr::opts_chunk$set(
  echo = TRUE,           # Show code in output
  eval = TRUE,           # Evaluate code chunks
  include = TRUE,        # Include chunk output
  message = FALSE,       # Suppress package messages
  warning = FALSE,       # Suppress warnings
  error = FALSE,         # Continue on errors (set to TRUE for debugging)
  cache = TRUE,          # Cache results for faster knitting
  cache.lazy = FALSE,    # Eager caching for large datasets
  fig.width = 10,        # Figure width in inches
  fig.height = 6,        # Figure height in inches
  fig.align = "center",  # Figure alignment
  fig.pos = "H",         # Figure position
  out.width = "90%",     # Output width
  fig.cap = TRUE,        # Enable figure captions
  dev = "png",           # Graphics device
  dpi = 300,             # Figure resolution
  tidy = FALSE,          # Don't tidy code (preserve formatting)
  results = "markup"     # How to display results
)


# Load required packages quietly
suppressPackageStartupMessages({
  library(bib2df)        # BibTeX file processing
  library(dplyr)         # Data manipulation
  library(tidyr)         # Data tidying
  library(stringr)       # String operations
  library(purrr)         # Functional programming
  library(ggplot2)       # Data visualization
  library(scales)        # Plot scales
  library(kableExtra)    # Enhanced tables
  library(DT)            # Interactive tables
  library(ggthemes)      # Additional ggplot themes
})

# Generate bibliography for all used packages
packages_to_cite <- c("bib2df", 
                      "dplyr", "tidyr", 
                      "stringr", "purrr", 
                      "ggplot2", 
                     "knitr", "bookdown", "DT")
knitr::write_bib(packages_to_cite, "bib/packages.bib")

# Set ggplot theme for consistent styling
theme_set(theme_minimal(base_size = 12, base_family = "sans"))
theme_update(
  plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
  plot.subtitle = element_text(face = "italic", size = 12, hjust = 0.5),
  axis.title = element_text(face = "bold", size = 12),
  legend.title = element_text(face = "bold"),
  panel.grid.minor = element_blank(),
  plot.caption = element_text(face = "italic", size = 10)
)

# Custom color palette for methods
method_colors <- c(
  bayesian = "#1f77b4",
  simulation = "#ff7f0e", 
  optimization = "#2ca02c",
  statistical = "#d62728",
  decision_analysis = "#9467bd",
  machine_learning = "#8c564b",
  multi_criteria = "#e377c2",
  economic_evaluation = "#7f7f7f",
  risk_analysis = "#bcbd22",
  forecasting = "#17becf"
)

```

```{r child="01_Introduction.Rmd"}
```

```{r child = "01_1_Review_and_Synthesis.rmd"}
```

# Data Import and Preprocessing

## Reading Bibliographic Data

```{r read_data, echo=TRUE}
# Read the bib file
bib_data <- bib2df::bib2df("bib/23_Methods_Review_Holistic_Systems.bib")
```

**Rationale:** We import bibliographic records using the `bib2df` package [@R-bib2df], which converts BibTeX files into structured data frames. This enables systematic processing of large literature collections (n = `r nrow(bib_data)`) while preserving metadata integrity.

## Data Cleaning and Feature Engineering

```{r clean_data, echo=TRUE}
# Basic cleaning and text preparation
clean_bib_data <- bib_data %>%
  dplyr::mutate(
    has_annotation = !is.na(ANNOTE) & ANNOTE != "",
    annotation_text = ifelse(has_annotation, ANNOTE, ""),
    full_text = paste(TITLE, ANNOTE, sep = " "),
    year = as.numeric(YEAR)
  )
```

**Methodology:** Using `dplyr` [@R-dplyr], we create analysis-ready features:

- **Annotation tracking**: Identify `r sum(clean_bib_data$has_annotation)` annotated papers for validation
- **Text consolidation**: Combine titles and annotations for method detection
- **Temporal normalization**: Convert publication years for trend analysis

This preprocessing ensures consistent data structure for automated classification of methodological approaches.

```{r method_categories}
# method categories
method_categories <- list(
  bayesian = c("bayesian", "bayes", "mcmc", "markov chain", "prior", "posterior"),
  simulation = c("simulation", "monte carlo", "stochastic", "agent-based", "discrete event"),
  optimization = c("optimization", "linear programming", "nonlinear programming", "heuristic"),
  statistical = c("regression", "anova", "time series", "survival analysis", "mixed model"),
  decision_analysis = c("decision analysis", "decision tree", "markov model", "value of information", "voi"),
  machine_learning = c("machine learning", "neural network", "random forest", "svm", "clustering"),
  multi_criteria = c("multi-criteria", "multi criteria", "analytic hierarchy", "ahp", "topsis"),
  economic_evaluation = c("cost-effectiveness", "cost-benefit", "cost-utility", "economic evaluation"),
  risk_analysis = c("risk analysis", "risk assessment", "sensitivity analysis", "uncertainty analysis")
)
```

## Automated Method Detection

```{r detect_methods}
source("R/detect_methods_enhanced.R")
# Apply method detection
clean_bib_data <- clean_bib_data %>%
  mutate(
    detected_methods = map_chr(full_text, detect_methods_enhanced),
    methods_from_title = map_chr(TITLE, detect_methods_enhanced)
  )
```

**Method:** We apply automated classification using a custom dictionary-based algorithm (`detect_methods_enhanced.R`) that identifies methodological approaches from text content. The `purrr` package [@R-purrr] enables efficient vectorized processing across all records.

**Purpose:** Dual detection (full text and titles only) enables confidence scoring, where methods appearing in titles receive higher reliability weights for subsequent analysis.

## Confidence Scoring

```{r add_confidence_scores}
source("R/add_confidence_scores.R")
# Apply confidence scoring
clean_bib_data <- add_confidence_scores(clean_bib_data)
```

**Method:** We assign confidence levels to method classifications based on detection location. Methods identified in titles receive higher confidence than those found only in annotations, enabling quality-weighted analysis.

**Purpose:** Confidence scoring prioritizes reliable classifications for key findings while maintaining comprehensive coverage, balancing precision and recall in automated text analysis.

## Method Detection Validation

```{r method_detection_check}
# Quick check of what we detected
cat("=== METHOD DETECTION SUMMARY ===\n")
cat("Total papers:", nrow(clean_bib_data), "\n")
cat("Papers with methods detected:", sum(clean_bib_data$detected_methods != ""), "\n")
cat("Papers with VOI:", sum(clean_bib_data$has_voi), "\n")
cat("Confidence distribution:\n")
print(table(clean_bib_data$method_confidence))
```

**Quality Control:** We validate the automated classification by reporting key metrics including detection rates and confidence level distribution.

**Purpose:** This summary ensures methodological transparency and provides immediate insight into classification performance before detailed analysis.

## Classification Examples

```{r sample_results}
# Show some examples
cat("\n=== SAMPLE OF DETECTED METHODS ===\n")
sample_results <- clean_bib_data %>%
  filter(detected_methods != "") %>%
  select(TITLE, detected_methods, method_confidence) %>%
  head(10)

print(sample_results)
```

**Validation:** We display representative examples of automated method classifications to illustrate detection accuracy and confidence assignment.

**Purpose:** Sample inspection provides face validity assessment and helps identify potential classification patterns or issues before full-scale analysis.

## Method Frequency Analysis

```{r method_results_plot}
source("R/analyze_method_frequency.R")

method_results <- analyze_method_frequency(clean_bib_data)
print(method_results$plot)
print(method_results$counts)
```

**Analysis:** We quantify and visualize the distribution of methodological approaches across the literature using frequency analysis and bar chart visualization via `ggplot2` [@R-ggplot2].

**Purpose:** This reveals dominant methodological paradigms and identifies less common approaches, providing the foundational landscape for understanding methodological preferences in decision support under uncertainty.

## Value of Information Analysis

```{r voi_results}
source("R/analyze_voi_papers.R")
# Analyze VOI papers
voi_results <- analyze_voi_papers(clean_bib_data)
```

**Focus:** We conduct specialized analysis of Value of Information (VOI) literature, examining methodological patterns and temporal trends within this key decision-support approach.

**Purpose:** This targeted analysis identifies methodological evolution and application contexts specific to VOI, informing recommendations for methodological integration in uncertainty-aware decision frameworks.

## Results Export

```{r results_table}
# Create a comprehensive results table
results_table <- clean_bib_data %>%
  select(
    TITLE, 
    YEAR = year,
    HAS_ANNOTATION = has_annotation,
    DETECTED_METHODS = detected_methods, 
    CONFIDENCE = method_confidence,
    VOI = has_voi,
    BAYESIAN = has_bayesian,
    SIMULATION = has_simulation,
    UNCERTAINTY = has_uncertainty
  )

# Export for manual verification
write.csv(results_table, "data/method_detection_results.csv", row.names = FALSE)
```

**Documentation:** We compile all classification results into a structured table and export for external validation and secondary analysis.

**Purpose:** This ensures full reproducibility, enables manual verification of automated classifications, and provides a foundation for future methodological studies using this annotated dataset.

## Analysis Summary

```{r summary_report}
# Create summary report
cat("\n=== EXPORT SUMMARY ===\n")
cat("Results exported to: method_detection_results.csv\n")
cat("Total papers processed:", nrow(clean_bib_data), "\n")
cat("Papers with methods:", sum(clean_bib_data$detected_methods != ""), "\n")
cat("High confidence classifications:", sum(clean_bib_data$method_confidence == "high"), "\n")
```

**Synthesis:** We provide a comprehensive summary of the automated classification pipeline performance and output metrics.

**Purpose:** This final summary quantifies analysis scope and classification reliability, establishing the foundation for subsequent methodological interpretation and research recommendations.

## Dominant Methodological Approaches

```{r top_methods}
# Show top methods
top_methods <- clean_bib_data %>%
  filter(detected_methods != "") %>%
  separate_rows(detected_methods, sep = "; ") %>%
  count(detected_methods, sort = TRUE) %>%
  head(10)

cat("\nTop 10 methods detected:\n")
print(top_methods)
```

**Findings:** We identify the most prevalent methodological approaches using frequency analysis and `tidyr` [@R-tidyr] for data restructuring.

**Purpose:** This reveals the methodological ecosystem's core components, highlighting established practices and potential gaps in decision support under uncertainty literature.

## VOI Method Validation

```{r validation_check}
# Quick validation - check if our detection makes sense
validation_check <- clean_bib_data %>%
  filter(has_voi) %>%
  select(TITLE, detected_methods, method_confidence) %>%
  arrange(desc(method_confidence))

cat("\n=== VALIDATION CHECK - VOI PAPERS ===\n")
print(validation_check)
```

**Validation:** We examine Value of Information (VOI) papers specifically to assess classification plausibility and confidence distribution within this focused methodological domain.

**Purpose:** This targeted validation ensures classification accuracy for the core methodological focus area, building confidence in automated detection for specialized methodological analysis.

## High-Reliability Classifications

```{r high_confidence}
# Check papers with high confidence
high_confidence <- clean_bib_data %>%
  filter(method_confidence == "high") %>%
  select(TITLE, detected_methods, methods_from_title)

cat("\n=== HIGH CONFIDENCE CLASSIFICATIONS ===\n")
print(high_confidence)
```

**Quality Assessment:** We examine the subset of classifications receiving the highest confidence rating, where methods were detected in both titles and full text.

**Purpose:** This validates the classification system's precision by reviewing its most reliable outputs, establishing trust in the automated methodology for key findings and conclusions.

## Interactive Reference Browser

```{r datatable_clean_bib_data}
# Minimal interactive table
DT::datatable(
  clean_bib_data %>%
    select(
      Author = AUTHOR,
      Year = YEAR,
      Title = TITLE
    ) %>%
    arrange(desc(Year), Author),
  options = list(
    pageLength = 5,
    lengthMenu = c(5, 10, 20),
    dom = 'tip'  # table, info, pagination only
  ),
  caption = "Reference List",
  rownames = FALSE,
  filter = "none"  # remove filter to save space
)
```

**Accessibility:** We provide an interactive reference browser using the `DT` package [@R-DT] for easy navigation of the analyzed literature collection.

**Purpose:** This enables readers to explore the source material directly, verifying context and supporting transparent engagement with the underlying bibliographic data.

```{r child="References.Rmd"}
```
